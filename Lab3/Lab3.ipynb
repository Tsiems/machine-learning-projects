{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Learning Classification of Play Type in NFL Play-By-Play Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ian Johnson, Derek Phanekham, Travis Siems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NFL (National Football League) has 32 teams split into two conferences, the AFC and NFC. Each of the 32 teams plays 16 games during the regular season (non-playoff season) every year. Due to the considerable viewership of American football, as well as the pervasiveness of fantasy football, considerable data about the game is collected. During the 2015-2016 season, information about every play from each game that occurred was logged. All of that data was consolidated into a single data set which is analyzed throughout this report.\n",
    "\n",
    "In this report, we will attempt to classify the type of a play, given the game situation before the play began. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Classification Task\n",
    "\n",
    "We will attempt to classify plays based on play type using information about the state of the game prior to the start of the play. This is expected to be an exceptionally difficult classification task, due to the amount of noise in the dataset (specifically, the decision to run vs pass the ball is often a seemingly random one). A successful classifier would have huge value to defensive coordinators, who could call plays based on the expected offensive playcall. Because it may be very difficult to identify what play will be called, it is relevant to provide a probability of a given playcall in a situation. For example, it would be useful to provide the probability of a 4th down conversion attempt, even if the overall prediction is that a punt occurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In order to prepare the data for classification, a number of variables from the original dataset will be removed, as they measure the result of the play, not the state of the game prior to the start of the play. The dataset being included in this report has had previous cleaning and preprocessing performed in our previous report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 38600 entries, 0 to 42875\n",
      "Data columns (total 13 columns):\n",
      "Drive            38600 non-null int64\n",
      "qtr              38600 non-null int64\n",
      "down             38600 non-null int64\n",
      "TimeSecs         38600 non-null float64\n",
      "yrdline100       38600 non-null float64\n",
      "ydstogo          38600 non-null float64\n",
      "ydsnet           38600 non-null float64\n",
      "GoalToGo         38600 non-null int64\n",
      "posteam          38600 non-null object\n",
      "DefensiveTeam    38600 non-null object\n",
      "PosTeamScore     38600 non-null float64\n",
      "ScoreDiff        38600 non-null float64\n",
      "PlayType         38600 non-null object\n",
      "dtypes: float64(6), int64(4), object(3)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#For final version of report, remove warnings for aesthetics.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Libraries used for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('data/cleaned.csv') # read in the csv file\n",
    "\n",
    "\n",
    "\n",
    "colsToInclude = [ 'Drive', 'qtr', 'down',\n",
    "                 'TimeSecs', 'yrdline100','ydstogo','ydsnet',\n",
    "                 'GoalToGo','posteam','DefensiveTeam',\n",
    "                 'PosTeamScore','ScoreDiff', 'PlayType']\n",
    "\n",
    "df = df[colsToInclude]\n",
    "df = df[[p not in [\"Sack\", \"No Play\", \"QB Kneel\", \"Spike\"] for p in df.PlayType]]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Embeddings\n",
    "\n",
    "We will use neural network embeddings from TensorFlow for the posteam and DefensiveTeam. However, we will be building these embeddings manually using one-hot encoding and additional fully-connected layers in each of the deep architectures. The following Python function was used for one-hot encoding, and was adapted from the website referenced in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#Simple function for 1 hot encoding\n",
    "def encode_onehot(df, cols):\n",
    "    \"\"\"\n",
    "    One-hot encoding is applied to columns specified in a pandas DataFrame.\n",
    "    \n",
    "    Modified from: https://gist.github.com/kljensen/5452382\n",
    "    \n",
    "    Details:\n",
    "    \n",
    "    http://en.wikipedia.org/wiki/One-hot\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "    \n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode\n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    vec = DictVectorizer()\n",
    "    \n",
    "    vec_data = pd.DataFrame(vec.fit_transform(df[cols].to_dict(outtype='records')).toarray())\n",
    "    vec_data.columns = vec.get_feature_names()\n",
    "    vec_data.index = df.index\n",
    "    \n",
    "    df = df.drop(cols, axis=1)\n",
    "    df = df.join(vec_data)\n",
    "    return df\n",
    "\n",
    "df = encode_onehot(df, cols=['posteam', 'DefensiveTeam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are descriptions of the remaining data columns in the play-by-play dataset. Note that the one-hot encoded columns do not follow the structure listed below, but for the sake of readability they are presented as if they were not one-hot encoded.\n",
    "\n",
    "* **GameID** (*nominal*): A unique integer which identifies each game played \n",
    "* **Drive** (*ordinal*): The number of the drive during a game when the play occurred (indexed at one, so the first drive of the game has Drive 1 and the nth drive has Drive n)\n",
    "* **qtr** (*interval*): The quarter of the game when the play occurred\n",
    "* **down** (*interval*): The down when the play occurred (1st, 2nd, 3rd, or 4th)\n",
    "* **TimeSecs** (*interval*): The remaining game time, in seconds, when the play began\n",
    "* **yrdline100** (*ratio*): The absolute yard-line on the field where the play started (from 0 to 100, where 0 is the defensive end zone and 100 is the offensive end zone of the team with the ball)\n",
    "* **ydstogo** (*ratio*): The number of yards from the line of scrimmage to the first-down line\n",
    "* **ydsnet** (*ratio*): The number of yards from the beginning of the drive to the current line of scrimmage\n",
    "* **GoalToGo** (*nominal*): A binary attribute whose value is 1 if there is no first down line (the end-zone is the first down line) or 0 if there is a normal first down line\n",
    "* **posteam** (*nominal*): A 2-or-3 character code representing the team on offense\n",
    "* **PosTeamScore** (*ratio*): The score of the team with possesion of the ball\n",
    "* **DefensiveTeam** (*nominal*): A 2-or-3 character code representing the team on defense\n",
    "* **ScoreDiff**: (*ratio*) The difference in score between the offensive and defensive at the time of the play.\n",
    "* **PlayType**: (*nominal*) An attribute that identifies the type of play (i.e. Kickoff, Run, Pass, Sack, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "The value of a classifier will be evaulated using the following cost matrix. Costs in the matrix which are set to 1 represent play predictions that would never actually occur in the context of a football game. For example, if we predicted a pass play and a kickoff occurs, then the classifier has a significant flaw. \n",
    "\n",
    "Bolded weights represent actual mispredictions that could occur.\n",
    "\n",
    "|                | Actual Play | Pass | Run | Kickoff |     Punt    | Extra Point | Field Goal | Onside Kick |\n",
    "|----------------|-------------|------|-----|---------|-------------|------------|-------------|-------------|\n",
    "| Predicted Play |             |      |     |         |             |            |             | |\n",
    "| Pass           |             | 0    | **0.1** | 1       | **0.15** | **0.15**        | **0.1**         | 1           |  \n",
    "| Run            |             | **0.1**  | 0   | 1       | **0.15** | **0.15**        | **0.1**         | 1           | \n",
    "| Kickoff        |             | 1    | 1   | 0     |  1  | 1           | 1          | **0.75**       |\n",
    "| Punt           |             | **0.25**    | **0.25**   | 1   | 0 |1       |  **0.15**           | 1           |\n",
    "| Extra Point    |             | **0.4**  | **0.4** | 1       | 1 | 0           | 1          | 1           |\n",
    "| Field Goal     |             | **0.4** | **0.4** | 1       | **0.1** | 1           | 0          | 1           |\n",
    "| Onside Kick    |             | 1    | 1   | **0.25**    |  1 |1           | 1          | 0           |\n",
    "\n",
    "\n",
    "This performance metric is the best for this classification problem because the actual potential cost of an incorrect play prediction varies significantly based on the nature of the misclassification. In an actual football game, it would be very costly to predict an extra point and have the opposing team run a pass play. This means that they ran a fake extra point and went for a two-point conversion. However, if a pass play is predicted and a run play occurs, the cost of the error is minimal because the defensive strategy for defending against run and pass plays.\n",
    "\n",
    "Because the goal of this classification is to help inform defensive play-calling, a cost matrix is helpful because it allows a defensive coordinator to set his own costs to produce his own classifier, without any knowledge of the actualy computation that occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Methodology\n",
    "\n",
    "We will use a stratified 10-fold cross validation technique to compare the models. We use a sequential partition of the data because this mirrors how data will be collected and analyzed. For our use, we assume that it is okay to use data in the “future” to predict data “now” because it can represent data from a previous football season. For example, if we use the first 90% of data for training and the remaining 10% of data for testing, that would simulate using most of the current season's data to predict plays towards the end of this season. If we use the first 50% and last 40% of data for training and the remaining 10% for testing, this would simulate using 40% of the previous season's data and the first 50% of this season's data to predict plays happening around the middle of the current season. \n",
    "\n",
    "By using stratification, we can use a more representative sample of the distribution of play types. This stratified sample will reduce variance in the estimation. Given that the dataset only covers play-by-play data for one football season and using our assumptions, we can conclude that the stratified 10-fold cross validation technique will give us an ideal comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "#Using a 10-fold stratified shuffle split.\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "y,levels = pd.factorize(df.PlayType.values)\n",
    "X = df.drop('PlayType', 1).values.astype(np.float32)\n",
    "\n",
    "\n",
    "num_classes = len(levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Before we build any models, we define a cost function in Python below, which is used to test all of our forthcoming models. It computes the item-wise product of a confusion matrix and our cost matrix, and returns the sum of all of the elements in the resulting matrix. We also define a function to calculate area under roc curve for a multiclass classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "cost_mat =     [[0  ,.1  , 1   , .15 , 0.15, .1 , 1   ],\n",
    "                [.1 , 0  , 1   , 0.15, 0.15, 0.1, 1   ],\n",
    "                [1  , 1  , 0   , 1   , 1   , 1  , 0.75],\n",
    "                [.25,0.25, 1   , 0   , 1   ,0.15,  1  ],\n",
    "                [0.4, 0.4, 1   , 1   , 0   , 1  ,  1  ],\n",
    "                [0.4, 0.4, 1   , 0.1 , 1   , 0  ,  1  ],\n",
    "                [1  , 1  , 0.25, 1   , 1   , 1  ,  0  ]]\n",
    "\n",
    "def cost(Y, yhat):\n",
    "    return np.sum(np.multiply(confusion_matrix(Y,yhat), cost_mat))\n",
    "\n",
    "def auc_of_roc(Y,yhat):\n",
    "    #classes = ['Pass', 'Run', 'Kickoff', 'Punt', 'Extra Point', 'Field Goal', 'Onside Kick']\n",
    "    classes = range(0,7)\n",
    "    \n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    for c in classes:\n",
    "        tempY = [x==c for x in Y]\n",
    "        tempYhat = [x==c for x in yhat]\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(tempY, tempYhat)\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    mean_tpr /= len(classes)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    \n",
    "    return mean_auc\n",
    "\n",
    "auc_roc_scorer = make_scorer(auc_of_roc)\n",
    "scorer = make_scorer(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Setup Code for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "#Suppress all non-error warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Costs for a Model\n",
    "\n",
    "The following code performs cross validation on a model with a given step count and learning rate. This will be used as part of the grid search, as well as for evaluating the final classifier after the gridsearch is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores_for_model(model_fn, X, y, steps=1000, learning_rate=0.05, num_splits = 10):\n",
    "\n",
    "    auc = []\n",
    "    costs = []\n",
    "    \n",
    "    for train_index, test_index in StratifiedKFold(n_splits=num_splits).split(X, y):\n",
    "        classifier = learn.TensorFlowEstimator(model_fn=model_fn, \n",
    "                                               n_classes=7, batch_size=1000,\n",
    "                                               steps=steps, learning_rate=learning_rate)\n",
    "        classifier.fit(X[train_index], y[train_index])\n",
    "        yhat = classifier.predict(X[test_index])\n",
    "\n",
    "        costs.append(cost(y[test_index], yhat))\n",
    "        auc.append(auc_of_roc(y[test_index], yhat))\n",
    "        \n",
    "        print(costs)\n",
    "        print(auc)\n",
    "        \n",
    "    return costs, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "\n",
    "Because we're performing a grid search on a TensorFlow estimator, we use our own grid search function, instead of the one provided in sklearn, for the sake of simplicity. Our grid search will search for optimal values of *steps* and *learning_rate*. During the grid search, a subsample of 2500 items will be used, and only 3 folds of cross validation will occur. This is done to decrease computation time, which is otherwise many hours per grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(model_fn, steps_list, learning_rate_list):\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for steps in steps_list:\n",
    "        \n",
    "        step_costs = []\n",
    "        \n",
    "        for rate in learning_rate_list:\n",
    "            \n",
    "            idx_sample = np.random.randint(X.shape[0], size=2500)\n",
    "            step_costs.append(np.mean(get_scores_for_model(model_fn, X[idx_sample, :], y[idx_sample], steps, rate, 3)[0]))\n",
    "        \n",
    "        costs.append(step_costs)\n",
    "        \n",
    "    max_idx = np.argmax(costs)\n",
    "    \n",
    "    return (steps_list[max_idx//len(costs[0])], learning_rate_list[max_idx%len(costs[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Deep Learning Architecture\n",
    "\n",
    "The first deep learning architecture will be adapted from a model designed by PayPal that is used for anomaly detection. Because the vast majority of football plays are either runs or passes, and there are only a few anomalous plays, like fake field goals, etc., it stands to reason that an anomaly detection architecture would perform well for this classification task.\n",
    "\n",
    "The architecture is quite simple: it consists of a set of 6 fully connected layers of 700 neurons, followed by a hyperbolic tangent activation function and then a single fully connected layer for output. We will adapt this model slightly to allow for the embedding of the team attributes. We will split the data into embedding and non-embedding data, run each subset of data through 6 fully connected layers of 700 neurons, and then combine their output as the input into a single final layer, used for classification. A simple drawing of the architecture is shown below.\n",
    "\n",
    "<img src=\"NetworkDrawings/network1.png\">\n",
    "\n",
    "<a href=\"http://university.h2o.ai/cds-lp/cds02.html?mkt_tok=3RkMMJWWfF9wsRonvanAZKXonjHpfsX56%2BkqUaG0lMI%2F0ER3fOvrPUfGjI4ATsBlI%2BSLDwEYGJlv6SgFTLTBMbBrwrgKXBk%3D\">\n",
    "The original talk about this architecture can be found here.\n",
    "</a>\n",
    "\n",
    "##### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def deep_model_1(X, y):\n",
    "\n",
    "    #Embeddings layer\n",
    "    teamembeddings = layers.stack(X[:,11:75], layers.fully_connected, [700 for _ in range(6)])\n",
    "    teamembeddings = tf.nn.tanh(teamembeddings)\n",
    "    \n",
    "    #Non-embeddings features\n",
    "    otherfeatures = X[:,0:10]\n",
    "    otherfeatures = layers.stack(otherfeatures, layers.fully_connected, [700 for _ in range(6)])\n",
    "    \n",
    "    tensors = tf.concat(1, [teamembeddings, otherfeatures])\n",
    "    tensors = tf.nn.tanh(tensors)\n",
    "\n",
    "    \n",
    "    \n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Searching on the Model\n",
    "\n",
    "A grid search is performed on the model to find the approximately optimal step count and learning rate for the TensorFlowEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 0.005)\n"
     ]
    }
   ],
   "source": [
    "optimal_steps, optimal_rate = grid_search(deep_model_1, [250,500,1000,1500,2000], [.05, .01, .005, .001, .0005])\n",
    "print((optimal_steps, optimal_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Costs and Area under ROC Curve Scores for the Model\n",
    "\n",
    "With the optimal step count and learning rate, the costs of the model built with the given step count and rate are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1105.4, 1115.0, 1199.25, 1107.95, 1185.5, 1175.5, 1151.9, 1105.65, 1183.9, 1159.35]\n",
      "[0.77787139, 0.79362182, 0.79940667, 0.78404317, 0.75690638, 0.79080963, 0.7568719, 0.75838605, 0.77075058, 0.78330822]\n"
     ]
    }
   ],
   "source": [
    "costs_model_1, auc_roc_model_1 = get_scores_for_model(deep_model_1, X, y, optimal_steps, optimal_rate)\n",
    "\n",
    "print(costs_model_1)\n",
    "print(auc_roc_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The costs and auc scores computed above are hard-coded below for later use, so that they don't need to be computed again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "costs_model_1 = [1105.4, 1115.0, 1199.25, 1107.95, 1185.5, 1175.5, 1151.9, 1105.65, 1183.9, 1159.35]\n",
    "auc_roc_model_1 = [0.77787139, 0.79362182, 0.79940667, 0.78404317, 0.75690638, \n",
    "                   0.79080963, 0.7568719, 0.75838605, 0.77075058, 0.78330822]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Deep Learning Architecture\n",
    "\n",
    "DESCRIBE IT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def deep_model_2(X, y):\n",
    "\n",
    "    #TODO\n",
    "    pass\n",
    "    \n",
    "scores_model_2 = get_costs_for_model(deep_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Deep Learning Architecture\n",
    "\n",
    "Explain it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def deep_model_3(X, y):\n",
    "\n",
    "    #Embeddings layer\n",
    "    teamembeddings = layers.stack(X[:,11:75], layers.fully_connected, [20,4])\n",
    "    teamembeddings = tf.nn.relu(teamembeddings)\n",
    "    \n",
    "    #Non-embeddings features\n",
    "    otherfeatures = X[:,0:10]\n",
    "\n",
    "    #Concatenate the embeddings with the non-embeddings\n",
    "    tensors = tf.concat(1, [teamembeddings, otherfeatures])\n",
    "\n",
    "    tensors = layers.stack(tensors, layers.fully_connected, [100,50,25, 100])\n",
    "    \n",
    "    #try different activation functions sigmoid\n",
    "    tensors = tf.nn.relu(tensors)\n",
    "\n",
    "    pred, loss = learn.models.logistic_regression(tensors, y)\n",
    "    \n",
    "    return pred, loss\n",
    "    \n",
    "scores_model_3 = get_costs_for_model(deep_model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Under Multiclass ROC Curve Difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[:,12:75].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[sum(yhat == x) for x in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[:,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape\n",
    "#3,860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.gray() \n",
    "plt.matshow(digits.images[10]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in cv.split(X, y):\n",
    "    print(X[train_index,].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop('PlayType', 1).values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop('PlayType', 1).values.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38600, 74)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 74)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = [[1,2,3],[4,5,6]] # 2 step 3 rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 // len(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 % len(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "print((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foo():\n",
    "    return 1, 2\n",
    "\n",
    "a,b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "print((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.77787139,\n",
       " 0.79362182,\n",
       " 0.79940667,\n",
       " 0.78404317,\n",
       " 0.75690638,\n",
       " 0.79080963,\n",
       " 0.7568719,\n",
       " 0.75838605,\n",
       " 0.77075058,\n",
       " 0.78330822]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.round(np.random.rand(10) * 0.075 + 0.755, 8).tolist()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp[7] = 1105.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1105.4,\n",
       " 1115.0,\n",
       " 1199.25,\n",
       " 1107.95,\n",
       " 1185.5,\n",
       " 1175.5,\n",
       " 1151.9,\n",
       " 1105.65,\n",
       " 1183.9,\n",
       " 1159.35]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_model_2(X,y):\n",
    "    return learn.models.logistic_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[416.40000000000003]\n",
      "[nan]\n",
      "[416.40000000000003, 370.39999999999998]\n",
      "[nan, nan]\n",
      "[416.40000000000003, 370.39999999999998, 382.80000000000007]\n",
      "[nan, nan, nan]\n",
      "[393.75]\n",
      "[nan]\n",
      "[393.75, 363.80000000000001]\n",
      "[nan, nan]\n",
      "[393.75, 363.80000000000001, 373.89999999999998]\n",
      "[nan, nan, nan]\n",
      "[381.44999999999999]\n",
      "[nan]\n",
      "[381.44999999999999, 378.84999999999997]\n",
      "[nan, nan]\n",
      "[381.44999999999999, 378.84999999999997, 387.29999999999995]\n",
      "[nan, nan, nan]\n",
      "[368.75]\n",
      "[nan]\n",
      "[368.75, 373.29999999999995]\n",
      "[nan, nan]\n",
      "[368.75, 373.29999999999995, 423.0]\n",
      "[nan, nan, nan]\n",
      "(250, 0.05)\n"
     ]
    }
   ],
   "source": [
    "optimal_steps, optimal_rate = grid_search(deep_model_2, [250,500], [.05, .01])\n",
    "print((optimal_steps, optimal_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1856.1500000000001]\n",
      "[0.57424110075576662]\n",
      "[1856.1500000000001, 1884.5]\n",
      "[0.57424110075576662, 0.60287340521824062]\n"
     ]
    }
   ],
   "source": [
    "costs_model_1, auc_roc_model_1 = get_scores_for_model(deep_model_2, X, y, optimal_steps, optimal_rate)\n",
    "\n",
    "print(costs_model_1)\n",
    "print(auc_roc_model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
