{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Learning Classification of Play Type in NFL Play-By-Play Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ian Johnson, Derek Phanekham, Travis Siems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NFL (National Football League) has 32 teams split into two conferences, the AFC and NFC. Each of the 32 teams plays 16 games during the regular season (non-playoff season) every year. Due to the considerable viewership of American football, as well as the pervasiveness of fantasy football, considerable data about the game is collected. During the 2015-2016 season, information about every play from each game that occurred was logged. All of that data was consolidated into a single data set which is analyzed throughout this report.\n",
    "\n",
    "In this report, we will attempt to classify the type of a play, given the game situation before the play began. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Classification Task\n",
    "\n",
    "We will attempt to classify plays based on play type using information about the state of the game prior to the start of the play. This is expected to be an exceptionally difficult classification task, due to the amount of noise in the dataset (specifically, the decision to run vs pass the ball is often a seemingly random one). A successful classifier would have huge value to defensive coordinators, who could call plays based on the expected offensive playcall. Because it may be very difficult to identify what play will be called, it is relevant to provide a probability of a given playcall in a situation. For example, it would be useful to provide the probability of a 4th down conversion attempt, even if the overall prediction is that a punt occurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In order to prepare the data for classification, a number of variables from the original dataset will be removed, as they measure the result of the play, not the state of the game prior to the start of the play. The dataset being included in this report has had previous cleaning and preprocessing performed in our previous report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 38600 entries, 0 to 42875\n",
      "Data columns (total 14 columns):\n",
      "GameID           38600 non-null int64\n",
      "Drive            38600 non-null int64\n",
      "qtr              38600 non-null int64\n",
      "down             38600 non-null int64\n",
      "TimeSecs         38600 non-null float64\n",
      "yrdline100       38600 non-null float64\n",
      "ydstogo          38600 non-null float64\n",
      "ydsnet           38600 non-null float64\n",
      "GoalToGo         38600 non-null int64\n",
      "posteam          38600 non-null object\n",
      "DefensiveTeam    38600 non-null object\n",
      "PosTeamScore     38600 non-null float64\n",
      "ScoreDiff        38600 non-null float64\n",
      "PlayType         38600 non-null object\n",
      "dtypes: float64(6), int64(5), object(3)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#For final version of report, remove warnings for aesthetics.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Libraries used for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('data/cleaned.csv') # read in the csv file\n",
    "\n",
    "colsToInclude = ['GameID', 'Drive', 'qtr', 'down',\n",
    "                 'TimeSecs', 'yrdline100','ydstogo','ydsnet',\n",
    "                 'GoalToGo','posteam','DefensiveTeam',\n",
    "                 'PosTeamScore','ScoreDiff', 'PlayType']\n",
    "\n",
    "df = df[colsToInclude]\n",
    "df = df[[p not in [\"Sack\", \"No Play\", \"QB Kneel\", \"Spike\"] for p in df.PlayType]]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Embeddings\n",
    "\n",
    "We will no longer use one-hot encoding for categorical attributes. Instead, we will use neural network embeddings from TensorFlow for the posteam and DefensiveTeam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "posteam = tf.contrib.layers.sparse_column_with_hash_bucket(\"posteam\", hash_bucket_size=100)\n",
    "defteam = tf.contrib.layers.sparse_column_with_hash_bucket(\"DefensiveTeam\", hash_bucket_size=100)\n",
    "\n",
    "#posteam = layers.embedding_column(posteam, dimension=8)\n",
    "#defteam = layers.embedding_column(defteam, dimension=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are descriptions of the remaining data columns in the play-by-play dataset. Note that the one-hot encoded columns do not follow the structure listed below, but for the sake of readability they are presented as if they were not one-hot encoded.\n",
    "\n",
    "* **GameID** (*nominal*): A unique integer which identifies each game played \n",
    "* **Drive** (*ordinal*): The number of the drive during a game when the play occurred (indexed at one, so the first drive of the game has Drive 1 and the nth drive has Drive n)\n",
    "* **qtr** (*interval*): The quarter of the game when the play occurred\n",
    "* **down** (*interval*): The down when the play occurred (1st, 2nd, 3rd, or 4th)\n",
    "* **TimeSecs** (*interval*): The remaining game time, in seconds, when the play began\n",
    "* **yrdline100** (*ratio*): The absolute yard-line on the field where the play started (from 0 to 100, where 0 is the defensive end zone and 100 is the offensive end zone of the team with the ball)\n",
    "* **ydstogo** (*ratio*): The number of yards from the line of scrimmage to the first-down line\n",
    "* **ydsnet** (*ratio*): The number of yards from the beginning of the drive to the current line of scrimmage\n",
    "* **GoalToGo** (*nominal*): A binary attribute whose value is 1 if there is no first down line (the end-zone is the first down line) or 0 if there is a normal first down line\n",
    "* **posteam** (*nominal*): A 2-or-3 character code representing the team on offense\n",
    "* **PosTeamScore** (*ratio*): The score of the team with possesion of the ball\n",
    "* **DefensiveTeam** (*nominal*): A 2-or-3 character code representing the team on defense\n",
    "* **ScoreDiff**: (*ratio*) The difference in score between the offensive and defensive at the time of the play.\n",
    "* **PlayType**: (*nominal*) An attribute that identifies the type of play (i.e. Kickoff, Run, Pass, Sack, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "The value of a classifier will be evaulated using the following cost matrix. Costs in the matrix which are set to 1 represent play predictions that would never actually occur in the context of a football game. For example, if we predicted a pass play and a kickoff occurs, then the classifier has a significant flaw. \n",
    "\n",
    "Bolded weights represent actual mispredictions that could occur.\n",
    "\n",
    "|                | Actual Play | Pass | Run | Kickoff |     Punt    | Extra Point | Field Goal | Onside Kick |\n",
    "|----------------|-------------|------|-----|---------|-------------|------------|-------------|-------------|\n",
    "| Predicted Play |             |      |     |         |             |            |             | |\n",
    "| Pass           |             | 0    | **0.1** | 1       | **0.15** | **0.15**        | **0.1**         | 1           |  \n",
    "| Run            |             | **0.1**  | 0   | 1       | **0.15** | **0.15**        | **0.1**         | 1           | \n",
    "| Kickoff        |             | 1    | 1   | 0     |  1  | 1           | 1          | **0.75**       |\n",
    "| Punt           |             | **0.25**    | **0.25**   | 1   | 0 |1       |  **0.15**           | 1           |\n",
    "| Extra Point    |             | **0.4**  | **0.4** | 1       | 1 | 0           | 1          | 1           |\n",
    "| Field Goal     |             | **0.4** | **0.4** | 1       | **0.1** | 1           | 0          | 1           |\n",
    "| Onside Kick    |             | 1    | 1   | **0.25**    |  1 |1           | 1          | 0           |\n",
    "\n",
    "\n",
    "This performance metric is the best for this classification problem because the actual potential cost of an incorrect play prediction varies significantly based on the nature of the misclassification. In an actual football game, it would be very costly to predict an extra point and have the opposing team run a pass play. This means that they ran a fake extra point and went for a two-point conversion. However, if a pass play is predicted and a run play occurs, the cost of the error is minimal because the defensive strategy for defending against run and pass plays.\n",
    "\n",
    "Because the goal of this classification is to help inform defensive play-calling, a cost matrix is helpful because it allows a defensive coordinator to set his own costs to produce his own classifier, without any knowledge of the actualy computation that occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Methodology\n",
    "\n",
    "We will use a stratified 10-fold cross validation technique to compare the models. We use a sequential partition of the data because this mirrors how data will be collected and analyzed. For our use, we assume that it is okay to use data in the “future” to predict data “now” because it can represent data from a previous football season. For example, if we use the first 90% of data for training and the remaining 10% of data for testing, that would simulate using most of the current season's data to predict plays towards the end of this season. If we use the first 50% and last 40% of data for training and the remaining 10% for testing, this would simulate using 40% of the previous season's data and the first 50% of this season's data to predict plays happening around the middle of the current season. \n",
    "\n",
    "By using stratification, we can use a more representative sample of the distribution of play types. This stratified sample will reduce variance in the estimation. Given that the dataset only covers play-by-play data for one football season and using our assumptions, we can conclude that the stratified 10-fold cross validation technique will give us an ideal comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "#Using a 10-fold stratified shuffle split.\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "y = df.PlayType.values\n",
    "X = df.drop('PlayType', 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Before we build any models, we define a cost function in Python below, which is used to test all of our forthcoming models. It computes the item-wise product of a confusion matrix and our cost matrix, and returns the sum of all of the elements in the resulting matrix. We also define a function to calculate area under roc curve for a multiclass classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "cost_mat =     [[0  ,.1  , 1   , .15 , 0.15, .1 , 1   ],\n",
    "                [.1 , 0  , 1   , 0.15, 0.15, 0.1, 1   ],\n",
    "                [1  , 1  , 0   , 1   , 1   , 1  , 0.75],\n",
    "                [.25,0.25, 1   , 0   , 1   ,0.15,  1  ],\n",
    "                [0.4, 0.4, 1   , 1   , 0   , 1  ,  1  ],\n",
    "                [0.4, 0.4, 1   , 0.1 , 1   , 0  ,  1  ],\n",
    "                [1  , 1  , 0.25, 1   , 1   , 1  ,  0  ]]\n",
    "\n",
    "def cost(Y, yhat):\n",
    "    return np.sum(np.multiply(confusion_matrix(Y,yhat), cost_mat))\n",
    "\n",
    "def auc_of_roc(Y,yhat):\n",
    "    classes = ['Pass', 'Run', 'Kickoff', 'Punt', 'Extra Point', 'Field Goal', 'Onside Kick']\n",
    "    \n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    for c in classes:\n",
    "        tempY = [x==c for x in Y]\n",
    "        tempYhat = [x==c for x in yhat]\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(tempY, tempYhat)\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=c+' (area = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Luck')\n",
    "\n",
    "    mean_tpr /= len(classes)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    print(mean_auc)\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_auc\n",
    "\n",
    "auc_roc_scorer = make_scorer(auc_of_roc)\n",
    "scorer = make_scorer(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Accessory Plotting Function\n",
    "\n",
    "The following plotting function will be used for tuning hyperparameters for various algorithms.\n",
    "It is adapted from one of the instructional ML notebooks for CSE 5393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Adapted From MachineLearningNotebooks/09_Evaluation\n",
    "#Credit: Dr. Eric Larson\n",
    "def plot_filled(train_scores,test_scores,train_x_axis, xlabel=''):\n",
    "\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.plot(train_x_axis, test_mean,\n",
    "             color='blue', marker='o',\n",
    "             markersize=5, label='testing cost')\n",
    "\n",
    "    plt.fill_between(train_x_axis,\n",
    "                     test_mean + test_std,\n",
    "                     test_mean - test_std,\n",
    "                     alpha=0.15, color='blue')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Cost')\n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Deep Learning Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/hp/xntg8b0x26777x7ft3n6yfgr0000gn/T/tmp_xlt69ul\n",
      "WARNING:tensorflow:Using default config.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Tensor(\"input:0\", shape=(?, 13), dtype=string)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument must be a dense tensor: _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='posteam', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x1221f4ea0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None) - got shape [8], but wanted [8, 6].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-026410641ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, steps, monitors, logdir)\u001b[0m\n\u001b[1;32m    255\u001b[0m                       \u001b[0mfeed_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_feeder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                       \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                       monitors=monitors)\n\u001b[0m\u001b[1;32m    258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\u001b[0m\n\u001b[1;32m    660\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m       \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m       \u001b[0;31m# Add default monitors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_get_train_ops\u001b[0;34m(self, features, targets)\u001b[0m\n\u001b[1;32m    961\u001b[0m       \u001b[0mTuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \"\"\"\n\u001b[0;32m--> 963\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, targets, mode)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, targets, mode)\u001b[0m\n\u001b[1;32m    491\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'class_weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m       \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrib_framework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-026410641ef7>\u001b[0m in \u001b[0;36mdeep_model\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposteam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(inputs, layer, stack_args, **kwargs)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stack_args need to be a list or tuple'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Stack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    178\u001b[0m                                          as_ref=False):\n\u001b[1;32m    179\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 163\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n\u001b[0m\u001b[1;32m    164\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m                          \"\"\" - got shape %s, but wanted %s.\"\"\" % (\n\u001b[1;32m    361\u001b[0m                              \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnparray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                              _GetDenseDimensions(values)))\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;31m# python/numpy default float type is float64. We prefer float32 instead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Argument must be a dense tensor: _EmbeddingColumn(sparse_id_column=_SparseColumnHashed(column_name='posteam', is_integerized=False, bucket_size=100, lookup_config=None, combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<function truncated_normal_initializer.<locals>._initializer at 0x1221f4ea0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None) - got shape [8], but wanted [8, 6]."
     ]
    }
   ],
   "source": [
    "def deep_model(X, y):\n",
    "    print('===============================')\n",
    "    print(X)\n",
    "    \n",
    "    features = layers.embedding_column(posteam, dimension=8)\n",
    "    \n",
    "    features = layers.stack(features, layers.fully_connected, [50])\n",
    "    features = tf.nn.relu(features)\n",
    "\n",
    "    pred, loss = learn.models.logistic_regression(features, y)\n",
    "    print(pred)\n",
    "    print('===============================')\n",
    "    return pred, loss\n",
    "\n",
    "\n",
    "classifier = learn.TensorFlowEstimator(model_fn=deep_model, \n",
    "                                       n_classes=10, steps=1000, \n",
    "                                       learning_rate=0.05, batch_size=128)\n",
    "\n",
    "b, c = np.unique(y, return_inverse=True)\n",
    "\n",
    "classifier.fit(X, c)\n",
    "score = accuracy_score(c, classifier.predict(X))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Deep Learning Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Under Multiclass ROC Curve Difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2015091000, 1, 1, ..., 'NE', 0.0, 0.0],\n",
       "       [2015091000, 1, 1, ..., 'NE', 0.0, 0.0],\n",
       "       [2015091000, 1, 1, ..., 'NE', 0.0, 0.0],\n",
       "       ..., \n",
       "       [2016010310, 22, 4, ..., 'MIN', 13.0, -6.0],\n",
       "       [2016010310, 22, 4, ..., 'MIN', 13.0, -6.0],\n",
       "       [2016010310, 22, 4, ..., 'MIN', 13.0, -6.0]], dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
