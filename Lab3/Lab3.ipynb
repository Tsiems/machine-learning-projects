{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Learning Classification of Play Type in NFL Play-By-Play Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ian Johnson, Derek Phanekham, Travis Siems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NFL (National Football League) has 32 teams split into two conferences, the AFC and NFC. Each of the 32 teams plays 16 games during the regular season (non-playoff season) every year. Due to the considerable viewership of American football, as well as the pervasiveness of fantasy football, considerable data about the game is collected. During the 2015-2016 season, information about every play from each game that occurred was logged. All of that data was consolidated into a single data set which is analyzed throughout this report.\n",
    "\n",
    "In this report, we will attempt to classify the type of a play, given the game situation before the play began. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Classification Task\n",
    "\n",
    "We will attempt to classify plays based on play type using information about the state of the game prior to the start of the play. This is expected to be an exceptionally difficult classification task, due to the amount of noise in the dataset (specifically, the decision to run vs pass the ball is often a seemingly random one). A successful classifier would have huge value to defensive coordinators, who could call plays based on the expected offensive playcall. Because it may be very difficult to identify what play will be called, it is relevant to provide a probability of a given playcall in a situation. For example, it would be useful to provide the probability of a 4th down conversion attempt, even if the overall prediction is that a punt occurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In order to prepare the data for classification, a number of variables from the original dataset will be removed, as they measure the result of the play, not the state of the game prior to the start of the play. The dataset being included in this report has had previous cleaning and preprocessing performed in our previous report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 38600 entries, 0 to 42875\n",
      "Data columns (total 13 columns):\n",
      "Drive            38600 non-null int64\n",
      "qtr              38600 non-null int64\n",
      "down             38600 non-null int64\n",
      "TimeSecs         38600 non-null float64\n",
      "yrdline100       38600 non-null float64\n",
      "ydstogo          38600 non-null float64\n",
      "ydsnet           38600 non-null float64\n",
      "GoalToGo         38600 non-null int64\n",
      "posteam          38600 non-null object\n",
      "DefensiveTeam    38600 non-null object\n",
      "PosTeamScore     38600 non-null float64\n",
      "ScoreDiff        38600 non-null float64\n",
      "PlayType         38600 non-null object\n",
      "dtypes: float64(6), int64(4), object(3)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#For final version of report, remove warnings for aesthetics.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Libraries used for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('data/cleaned.csv') # read in the csv file\n",
    "\n",
    "\n",
    "\n",
    "colsToInclude = [ 'Drive', 'qtr', 'down',\n",
    "                 'TimeSecs', 'yrdline100','ydstogo','ydsnet',\n",
    "                 'GoalToGo','posteam','DefensiveTeam',\n",
    "                 'PosTeamScore','ScoreDiff', 'PlayType']\n",
    "\n",
    "df = df[colsToInclude]\n",
    "df = df[[p not in [\"Sack\", \"No Play\", \"QB Kneel\", \"Spike\"] for p in df.PlayType]]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Embeddings\n",
    "\n",
    "We will use neural network embeddings from TensorFlow for the posteam and DefensiveTeam. However, we will be building these embeddings manually using one-hot encoding and additional fully-connected layers in each of the deep architectures. The following Python function was used for one-hot encoding, and was adapted from the website referenced in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#Simple function for 1 hot encoding\n",
    "def encode_onehot(df, cols):\n",
    "    \"\"\"\n",
    "    One-hot encoding is applied to columns specified in a pandas DataFrame.\n",
    "    \n",
    "    Modified from: https://gist.github.com/kljensen/5452382\n",
    "    \n",
    "    Details:\n",
    "    \n",
    "    http://en.wikipedia.org/wiki/One-hot\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "    \n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode\n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    vec = DictVectorizer()\n",
    "    \n",
    "    vec_data = pd.DataFrame(vec.fit_transform(df[cols].to_dict(outtype='records')).toarray())\n",
    "    vec_data.columns = vec.get_feature_names()\n",
    "    vec_data.index = df.index\n",
    "    \n",
    "    df = df.drop(cols, axis=1)\n",
    "    df = df.join(vec_data)\n",
    "    return df\n",
    "\n",
    "df = encode_onehot(df, cols=['posteam', 'DefensiveTeam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are descriptions of the remaining data columns in the play-by-play dataset. Note that the one-hot encoded columns do not follow the structure listed below, but for the sake of readability they are presented as if they were not one-hot encoded.\n",
    "\n",
    "* **GameID** (*nominal*): A unique integer which identifies each game played \n",
    "* **Drive** (*ordinal*): The number of the drive during a game when the play occurred (indexed at one, so the first drive of the game has Drive 1 and the nth drive has Drive n)\n",
    "* **qtr** (*interval*): The quarter of the game when the play occurred\n",
    "* **down** (*interval*): The down when the play occurred (1st, 2nd, 3rd, or 4th)\n",
    "* **TimeSecs** (*interval*): The remaining game time, in seconds, when the play began\n",
    "* **yrdline100** (*ratio*): The absolute yard-line on the field where the play started (from 0 to 100, where 0 is the defensive end zone and 100 is the offensive end zone of the team with the ball)\n",
    "* **ydstogo** (*ratio*): The number of yards from the line of scrimmage to the first-down line\n",
    "* **ydsnet** (*ratio*): The number of yards from the beginning of the drive to the current line of scrimmage\n",
    "* **GoalToGo** (*nominal*): A binary attribute whose value is 1 if there is no first down line (the end-zone is the first down line) or 0 if there is a normal first down line\n",
    "* **posteam** (*nominal*): A 2-or-3 character code representing the team on offense\n",
    "* **PosTeamScore** (*ratio*): The score of the team with possesion of the ball\n",
    "* **DefensiveTeam** (*nominal*): A 2-or-3 character code representing the team on defense\n",
    "* **ScoreDiff**: (*ratio*) The difference in score between the offensive and defensive at the time of the play.\n",
    "* **PlayType**: (*nominal*) An attribute that identifies the type of play (i.e. Kickoff, Run, Pass, Sack, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "The value of a classifier will be evaulated using the following cost matrix. Costs in the matrix which are set to 1 represent play predictions that would never actually occur in the context of a football game. For example, if we predicted a pass play and a kickoff occurs, then the classifier has a significant flaw. \n",
    "\n",
    "Bolded weights represent actual mispredictions that could occur.\n",
    "\n",
    "|                | Actual Play | Pass | Run | Kickoff |     Punt    | Extra Point | Field Goal | Onside Kick |\n",
    "|----------------|-------------|------|-----|---------|-------------|------------|-------------|-------------|\n",
    "| Predicted Play |             |      |     |         |             |            |             | |\n",
    "| Pass           |             | 0    | **0.1** | 1       | **0.15** | **0.15**        | **0.1**         | 1           |  \n",
    "| Run            |             | **0.1**  | 0   | 1       | **0.15** | **0.15**        | **0.1**         | 1           | \n",
    "| Kickoff        |             | 1    | 1   | 0     |  1  | 1           | 1          | **0.75**       |\n",
    "| Punt           |             | **0.25**    | **0.25**   | 1   | 0 |1       |  **0.15**           | 1           |\n",
    "| Extra Point    |             | **0.4**  | **0.4** | 1       | 1 | 0           | 1          | 1           |\n",
    "| Field Goal     |             | **0.4** | **0.4** | 1       | **0.1** | 1           | 0          | 1           |\n",
    "| Onside Kick    |             | 1    | 1   | **0.25**    |  1 |1           | 1          | 0           |\n",
    "\n",
    "\n",
    "This performance metric is the best for this classification problem because the actual potential cost of an incorrect play prediction varies significantly based on the nature of the misclassification. In an actual football game, it would be very costly to predict an extra point and have the opposing team run a pass play. This means that they ran a fake extra point and went for a two-point conversion. However, if a pass play is predicted and a run play occurs, the cost of the error is minimal because the defensive strategy for defending against run and pass plays.\n",
    "\n",
    "Because the goal of this classification is to help inform defensive play-calling, a cost matrix is helpful because it allows a defensive coordinator to set his own costs to produce his own classifier, without any knowledge of the actualy computation that occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Methodology\n",
    "\n",
    "We use a sequential k-fold partition of the data because this mirrors how data will be collected and analyzed. For our use, we assume that it is okay to use data in the “future” to predict data “now” because it can represent data from a previous football season. For example, if we use the first 90% of data for training and the remaining 10% of data for testing, that would simulate using most of the current season's data to predict plays towards the end of this season. If we use the first 50% and last 40% of data for training and the remaining 10% for testing, this would simulate using 40% of the previous season's data and the first 50% of this season's data to predict plays happening around the middle of the current season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Using a 10-fold sequential split.\n",
    "#Note that this cv object is unused, but is here for reference\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "y,levels = pd.factorize(df.PlayType.values)\n",
    "X = df.drop('PlayType', 1).values.astype(np.float32)\n",
    "\n",
    "\n",
    "num_classes = len(levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Before we build any models, we define a cost function in Python below, which is used to test all of our forthcoming models. It computes the item-wise product of a confusion matrix and our cost matrix, and returns the sum of all of the elements in the resulting matrix. We also define a function to calculate area under roc curve for a multiclass classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc,make_scorer\n",
    "from scipy import interp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "cost_mat =     [[0  ,.1  , 1   , .15 , 0.15, .1 , 1   ],\n",
    "                [.1 , 0  , 1   , 0.15, 0.15, 0.1, 1   ],\n",
    "                [1  , 1  , 0   , 1   , 1   , 1  , 0.75],\n",
    "                [.25,0.25, 1   , 0   , 1   ,0.15,  1  ],\n",
    "                [0.4, 0.4, 1   , 1   , 0   , 1  ,  1  ],\n",
    "                [0.4, 0.4, 1   , 0.1 , 1   , 0  ,  1  ],\n",
    "                [1  , 1  , 0.25, 1   , 1   , 1  ,  0  ]]\n",
    "\n",
    "def cost(Y, yhat):\n",
    "    return np.sum(np.multiply(confusion_matrix(Y,yhat), cost_mat))\n",
    "\n",
    "def auc_of_roc(Y,yhat):\n",
    "    #classes = ['Pass', 'Run', 'Kickoff', 'Punt', 'Extra Point', 'Field Goal', 'Onside Kick']\n",
    "    classes = range(0,7)\n",
    "    \n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    for c in classes:\n",
    "        tempY = [x==c for x in Y]\n",
    "        tempYhat = [x==c for x in yhat]\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(tempY, tempYhat)\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    mean_tpr /= len(classes)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    \n",
    "    return mean_auc\n",
    "\n",
    "auc_roc_scorer = make_scorer(auc_of_roc)\n",
    "scorer = make_scorer(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Setup Code for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "#Suppress all non-error warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Costs for a Model\n",
    "\n",
    "The following code performs cross validation on a model with a given step count and learning rate. This will be used as part of the grid search, as well as for evaluating the final classifier after the gridsearch is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores_for_model(model_fn, X, y, steps=1000, learning_rate=0.05, num_splits = 10):\n",
    "\n",
    "    auc = []\n",
    "    costs = []\n",
    "    \n",
    "    for train_index, test_index in KFold(n_splits=num_splits).split(X, y):\n",
    "        classifier = learn.TensorFlowEstimator(model_fn=model_fn, \n",
    "                                               n_classes=7, batch_size=1000,\n",
    "                                               steps=steps, learning_rate=learning_rate)\n",
    "        classifier.fit(X[train_index], y[train_index])\n",
    "        yhat = classifier.predict(X[test_index])\n",
    "\n",
    "        costs.append(cost(y[test_index], yhat))\n",
    "        auc.append(auc_of_roc(y[test_index], yhat))\n",
    "\n",
    "    return costs, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "\n",
    "Because we're performing a grid search on a TensorFlow estimator, we use our own grid search function, instead of the one provided in sklearn, for the sake of simplicity. Our grid search will search for optimal values of *steps* and *learning_rate*. During the grid search, a subsample of 2500 items will be used, and only 3 folds of cross validation will occur. This is done to decrease computation time, which is otherwise many hours per grid search.\n",
    "\n",
    "Note that the grid search function itself is not parallelized. This is because the underlying TensorFlow modelling is all parallelized, so maximal CPU usage is already being achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(model_fn, steps_list, learning_rate_list):\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for steps in steps_list:\n",
    "        \n",
    "        step_costs = []\n",
    "        \n",
    "        for rate in learning_rate_list:\n",
    "            \n",
    "            idx_sample = np.random.randint(X.shape[0], size=5000)\n",
    "            step_costs.append(np.mean(get_scores_for_model(model_fn, X[idx_sample, :], y[idx_sample], steps, rate, 3)[0]))\n",
    "        \n",
    "        costs.append(step_costs)\n",
    "        \n",
    "    max_idx = np.argmax(costs)\n",
    "    \n",
    "    return costs, steps_list[max_idx//len(costs[0])], learning_rate_list[max_idx%len(costs[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-94829740dcd1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-94829740dcd1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def grid_search_heatmap(matrix)\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def grid_search_heatmap(costs):\n",
    "    \n",
    "    plt.pcolor(np.array(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Deep Learning Architecture\n",
    "\n",
    "The first deep learning architecture will be adapted from a model designed by PayPal that is used for anomaly detection. Because the vast majority of football plays are either runs or passes, and there are only a few anomalous plays, like onside kicks, etc., it stands to reason that an anomaly detection architecture would perform well for this classification task.\n",
    "\n",
    "The architecture is quite simple: it consists of a set of 6 fully connected layers of 700 neurons, followed by a hyperbolic tangent activation function and then a single fully connected layer for output. We will adapt this model slightly to allow for the embedding of the team attributes. We will split the data into embedding and non-embedding data, run each subset of data through 6 fully connected layers of 700 neurons, and then combine their output as the input into a single final layer, used for classification. A simple drawing of the architecture is shown below.\n",
    "\n",
    "<img src=\"NetworkDrawings/network1.png\">\n",
    "\n",
    "<a href=\"http://university.h2o.ai/cds-lp/cds02.html?mkt_tok=3RkMMJWWfF9wsRonvanAZKXonjHpfsX56%2BkqUaG0lMI%2F0ER3fOvrPUfGjI4ATsBlI%2BSLDwEYGJlv6SgFTLTBMbBrwrgKXBk%3D\">\n",
    "The original talk about this architecture can be found here.\n",
    "</a>\n",
    "\n",
    "##### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def deep_model_1(X, y):\n",
    "\n",
    "    #Embeddings layer\n",
    "    teamembeddings = layers.stack(X[:,11:75], layers.fully_connected, [700 for _ in range(6)])\n",
    "    teamembeddings = tf.nn.tanh(teamembeddings)\n",
    "    \n",
    "    #Non-embeddings features\n",
    "    otherfeatures = X[:,0:10]\n",
    "    otherfeatures = layers.stack(otherfeatures, layers.fully_connected, [700 for _ in range(6)])\n",
    "    \n",
    "    tensors = tf.concat(1, [teamembeddings, otherfeatures])\n",
    "    tensors = tf.nn.tanh(tensors)\n",
    "    \n",
    "    pred,loss = learn.models.logistic_regression(tensors, y)\n",
    "\n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Searching on the Model\n",
    "\n",
    "A grid search is performed on the model to find the approximately optimal step count and learning rate for the TensorFlowEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "costs, optimal_steps, optimal_rate = grid_search(deep_model_1, [250,500,1000,1500,2000], [.05, .01, .005, .001, .0005])\n",
    "print((optimal_steps, optimal_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(optimal_steps)\n",
    "print(optimal_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the grid search returned the optimal step count and rate, it is meaningful to visualize the grid that was generated, to get an idea for how much better these particular hyperparameters are than the other possible combinations in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_heatmap(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Costs and Area under ROC Curve Scores for the Model\n",
    "\n",
    "With the optimal step count and learning rate, the costs of the model built with the given step count and rate are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1105.4, 1115.0, 1199.25, 1107.95, 1185.5, 1175.5, 1151.9, 1105.65, 1183.9, 1159.35]\n",
      "[0.77787139, 0.79362182, 0.79940667, 0.78404317, 0.75690638, 0.79080963, 0.7568719, 0.75838605, 0.77075058, 0.78330822]\n"
     ]
    }
   ],
   "source": [
    "costs_model_1, auc_roc_model_1 = get_scores_for_model(deep_model_1, X, y, optimal_steps, optimal_rate)\n",
    "\n",
    "print(costs_model_1)\n",
    "print(auc_roc_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The costs and auc scores computed above are hard-coded below for later use, so that they don't need to be computed again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "costs_model_1 = [1105.4, 1115.0, 1199.25, 1107.95, 1185.5, 1175.5, 1151.9, 1105.65, 1183.9, 1159.35]\n",
    "auc_roc_model_1 = [0.77787139, 0.79362182, 0.79940667, 0.78404317, 0.75690638, \n",
    "                   0.79080963, 0.7568719, 0.75838605, 0.77075058, 0.78330822]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Deep Learning Architecture\n",
    "\n",
    "DESCRIBE IT!\n",
    "\n",
    "Based on https://oshearesearch.com/index.php/tag/deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2042.75]\n",
      "[0.49999999999999994]\n",
      "[2042.75, 2007.5]\n",
      "[0.49999999999999994, 0.49999999999999994]\n",
      "[2042.75, 2007.5, 2008.75]\n",
      "[0.49999999999999994, 0.49999999999999994, 0.50008261353449202]\n",
      "[2042.75, 2007.5, 2008.75, 1978.0]\n",
      "[0.49999999999999994, 0.49999999999999994, 0.50008261353449202, 0.49999999999999994]\n",
      "[2042.75, 2007.5, 2008.75, 1978.0, 2059.5500000000002]\n",
      "[0.49999999999999994, 0.49999999999999994, 0.50008261353449202, 0.49999999999999994, 0.50000496933088179]\n",
      "[2042.75, 2007.5, 2008.75, 1978.0, 2059.5500000000002, 2021.75]\n",
      "[0.49999999999999994, 0.49999999999999994, 0.50008261353449202, 0.49999999999999994, 0.50000496933088179, 0.49999999999999994]\n",
      "[2042.75, 2007.5, 2008.75, 1978.0, 2059.5500000000002, 2021.75, 1974.0]\n",
      "[0.49999999999999994, 0.49999999999999994, 0.50008261353449202, 0.49999999999999994, 0.50000496933088179, 0.49999999999999994, 0.49999999999999994]\n",
      "[2042.75, 2007.5, 2008.75, 1978.0, 2059.5500000000002, 2021.75, 1974.0, 2048.25]\n",
      "[0.49999999999999994, 0.49999999999999994, 0.50008261353449202, 0.49999999999999994, 0.50000496933088179, 0.49999999999999994, 0.49999999999999994, 0.49999999999999994]\n",
      "[2042.75, 2007.5, 2008.75, 1978.0, 2059.5500000000002, 2021.75, 1974.0, 2048.25, 2027.0]\n",
      "[0.49999999999999994, 0.49999999999999994, 0.50008261353449202, 0.49999999999999994, 0.50000496933088179, 0.49999999999999994, 0.49999999999999994, 0.49999999999999994, 0.49999999999999994]\n",
      "[2042.75, 2007.5, 2008.75, 1978.0, 2059.5500000000002, 2021.75, 1974.0, 2048.25, 2027.0, 2041.25]\n",
      "[0.49999999999999994, 0.49999999999999994, 0.50008261353449202, 0.49999999999999994, 0.50000496933088179, 0.49999999999999994, 0.49999999999999994, 0.49999999999999994, 0.49999999999999994, 0.49999999999999994]\n",
      "CPU times: user 5min 45s, sys: 28.2 s, total: 6min 13s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def deep_model_2(X, y):\n",
    "    #Embeddings layer\n",
    "    teamembeddings = layers.stack(X[:,11:75], layers.fully_connected, [20,4])\n",
    "    teamembeddings = tf.nn.relu(teamembeddings)\n",
    "    \n",
    "    #Non-embeddings features\n",
    "    otherfeatures = X[:,0:10]\n",
    "\n",
    "    #Concatenate the embeddings with the non-embeddings\n",
    "    tensors = tf.concat(1, [teamembeddings, otherfeatures])\n",
    "\n",
    "    tensors = layers.stack(tensors, layers.fully_connected, [100,50,25, 100])\n",
    "    \n",
    "    #try different activation functions sigmoid\n",
    "    tensors = tf.nn.relu(tensors)\n",
    "    tensors = tf.nn.relu(tensors)\n",
    "    \n",
    "    tensors = tf.nn.softmax(tensors)\n",
    "\n",
    "    pred, loss = learn.models.logistic_regression(tensors, y)\n",
    "    \n",
    "    return pred, loss\n",
    "    \n",
    "scores_model_2 = get_scores_for_model(deep_model_2,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2042.75, 2007.5, 2008.75, 1978.0, 2059.5500000000002, 2021.75, 1974.0, 2048.25, 2027.0, 2041.25], [0.49999999999999994, 0.49999999999999994, 0.50008261353449202, 0.49999999999999994, 0.50000496933088179, 0.49999999999999994, 0.49999999999999994, 0.49999999999999994, 0.49999999999999994, 0.49999999999999994])\n"
     ]
    }
   ],
   "source": [
    "print(scores_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Deep Learning Architecture\n",
    "\n",
    "Explain it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6,6) (7,7) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-7995d7c05fb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\ndef deep_model_3(X, y):\\n\\n    #Embeddings layer\\n    teamembeddings = layers.stack(X[:,11:75], layers.fully_connected, [20,4])\\n    #teamembeddings = tf.nn.relu(teamembeddings)\\n    \\n    #Non-embeddings features\\n    otherfeatures = X[:,0:10]\\n\\n    #Concatenate the embeddings with the non-embeddings\\n    tensors = tf.concat(1, [teamembeddings, otherfeatures])\\n    \\n    #[500,200,100,500][1000,1000,1000,500,200,1000]\\n    tensors = layers.stack(tensors, layers.fully_connected, [1000,1000,1000,500,200,1000])\\n    \\n    #try different activation functions sigmoid\\n    tensors = tf.nn.relu(tensors)\\n    tensors = tf.nn.relu(tensors)\\n    tensors = tf.nn.relu(tensors)\\n\\n    pred, loss = learn.models.logistic_regression(tensors, y)\\n    \\n    return pred, loss\\n\\n\\noptimal_steps, optimal_rate = grid_search(deep_model_3, [100,250,500], [0.005, .0005, .00005])\\nprint(\"optimal steps: \", optimal_steps, \" optimal rate: \", optimal_rate)\\n    '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/derek/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/derek/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/derek/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-98319b9e55dd>\u001b[0m in \u001b[0;36mgrid_search\u001b[1;34m(model_fn, steps_list, learning_rate_list)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0midx_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mstep_costs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_scores_for_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_sample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mcosts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_costs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-048e34b85294>\u001b[0m in \u001b[0;36mget_scores_for_model\u001b[1;34m(model_fn, X, y, steps, learning_rate, num_splits)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mcosts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mauc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauc_of_roc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-72f804fdb069>\u001b[0m in \u001b[0;36mcost\u001b[1;34m(Y, yhat)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mauc_of_roc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6,6) (7,7) "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def deep_model_3(X, y):\n",
    "\n",
    "    #Embeddings layer\n",
    "    teamembeddings = layers.stack(X[:,11:75], layers.fully_connected, [20,4])\n",
    "    #teamembeddings = tf.nn.relu(teamembeddings)\n",
    "    \n",
    "    #Non-embeddings features\n",
    "    otherfeatures = X[:,0:10]\n",
    "\n",
    "    #Concatenate the embeddings with the non-embeddings\n",
    "    tensors = tf.concat(1, [teamembeddings, otherfeatures])\n",
    "    \n",
    "    #[500,200,100,500][1000,1000,1000,500,200,1000]\n",
    "    tensors = layers.stack(tensors, layers.fully_connected, [1000,1000,1000,500,200,1000])\n",
    "    \n",
    "    #try different activation functions sigmoid\n",
    "    tensors = tf.nn.relu(tensors)\n",
    "    tensors = tf.nn.relu(tensors)\n",
    "    tensors = tf.nn.relu(tensors)\n",
    "\n",
    "    pred, loss = learn.models.logistic_regression(tensors, y)\n",
    "    \n",
    "    return pred, loss\n",
    "\n",
    "\n",
    "optimal_steps, optimal_rate = grid_search(deep_model_3, [100,250,500], [0.005, .0005, .00005])\n",
    "print(\"optimal steps: \", optimal_steps, \" optimal rate: \", optimal_rate)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1654.9499999999998, 1546.3000000000002, 1583.3000000000002, 1547.2, 1640.95, 1634.7, 1665.25, 1626.7, 1678.3, 1639.5500000000002], [0.60113696461703936, 0.62256431559564995, 0.59913643227078495, 0.61860384436436899, 0.5933883723124902, 0.64275680858409689, 0.58600068196046851, 0.62421285285956252, 0.5873372252774246, 0.5973412561138246])\n"
     ]
    }
   ],
   "source": [
    "#optimal steps250  optimal rate:  5e-05\n",
    "#optimal_steps = 250\n",
    "#optimal_rate = .00005\n",
    "scores_model_3 = get_scores_for_model(deep_model_3,X,y)#,optimal_steps,optimal_rate)\n",
    "print(scores_model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Comparison\n",
    "\n",
    "The following getDifference function is used to create a tuple of the range of possible differences of mean for two sets of scores (cost or auc_roc), with 95% confidence. We use the second of the two confidence interval tests proposed in the ICA3 reversed assignment, because the datasets cannot be assumed to be independent, so the binomial approximation to the normal distribution does not hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDifference(cost1,cost2,z_val=2.26,size=10):\n",
    "    diff12 = cost1 - cost2\n",
    "    sigma12 = np.sqrt(np.sum(diff12*diff12) * 1/(size-1))\n",
    "    d12 = (np.mean(diff12) + 1/(np.sqrt(size)) * z_val * sigma12, np.mean(diff12) - 1/(np.sqrt(size)) * z_val * sigma12)\n",
    "    return d12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Difference\n",
    "\n",
    "The getDifference function is now used to create confidence intervals for the cost differences of the 3 possible pairs of two architectures created from the set of 3 architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_one_two = np.array(getDifference(costs_model_1, costs_model_2))\n",
    "d_one_three = np.array(getDifference(costs_model_1, costs_model_3))\n",
    "d_two_three = np.array(getDifference(costs_model_2, costs_model_3))\n",
    "\n",
    "print(\"Average Model 1 vs Model 2 Difference:\", d_one_two)\n",
    "print(\"Average Model 1 vs Model 3 Difference:\", d_one_three)\n",
    "print(\"Average Model 2 vs Model 3 Difference:\", d_two_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS SHOWS THAT... EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Under Multiclass ROC Curve Difference\n",
    "\n",
    "The same process is now used for confidence intervals for the auc_roc metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_one_two = np.array(getDifference(auc_roc_model_1, auc_roc_model_2))\n",
    "d_one_three = np.array(getDifference(auc_roc_model_1, auc_roc_model_3))\n",
    "d_two_three = np.array(getDifference(auc_roc_model_2, auc_roc_model_3))\n",
    "\n",
    "print(\"Average Model 1 vs Model 2 Difference:\", d_one_two)\n",
    "print(\"Average Model 1 vs Model 3 Difference:\", d_one_three)\n",
    "print(\"Average Model 2 vs Model 3 Difference:\", d_two_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS SHOWS THAT... EXPLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "HOW ARE WE GOING TO DEPLOY? CAN WE EVEN BEAT RANDOM FORESTS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
